diff --git a/trl/trainer/grpo_trainer.py b/grpo_trainer_updated.py
index fe29c78..e339152 100644
--- a/trl/trainer/grpo_trainer.py
+++ b/grpo_trainer_updated.py
@@ -400,6 +400,9 @@ class GRPOTrainer(BaseTrainer):
         self.importance_sampling_level = args.importance_sampling_level
         self.mask_truncated_completions = args.mask_truncated_completions
         self.top_entropy_quantile = args.top_entropy_quantile
+        self.smoothing_alpha = args.smoothing_alpha
+        self.trust_region_method = args.trust_region_method
+
         if self.use_liger_kernel and self.top_entropy_quantile < 1.0:
             raise NotImplementedError(
                 "Liger Kernels don't currently support masking token positions based on entropy."
@@ -1820,15 +1823,30 @@ class GRPOTrainer(BaseTrainer):
         # importance_sampling_level: "token" level: (B, T); "sequence" level: (B, 1)
 
         coef_1 = torch.exp(log_importance_weights)
-        coef_2 = torch.clamp(coef_1, 1 - self.epsilon_low, 1 + self.epsilon_high)
 
-        # Two-sided clipping
-        if self.args.delta is not None:
-            coef_1 = torch.clamp(coef_1, max=self.args.delta)
+        if self.trust_region_method == 'clip':
+            coef_2 = torch.clamp(coef_1, 1 - self.epsilon_low, 1 + self.epsilon_high)
+
+            # Two-sided clipping
+            if self.args.delta is not None:
+                coef_1 = torch.clamp(coef_1, max=self.args.delta)
+
+            per_token_loss1 = coef_1 * advantages.unsqueeze(1)
+
+            per_token_loss2 = coef_2 * advantages.unsqueeze(1)
+            per_token_loss = -torch.min(per_token_loss1, per_token_loss2)
+
+        elif self.trust_region_method == "pspo":
+            # smooth the ratio (equiv. to smoothing prob when smoothing toward behaviour policy)
+            coef_2 = (1.0 - self.smoothing_alpha)*coef_1 + self.smoothing_alpha
+            per_token_loss1 = None
+            per_token_loss2 = None
+            # always use smoothing for loss (no min)
+            per_token_loss = -(coef_2 * advantages.unsqueeze(1))
+    
+        else:
+            raise AttributeError(f'Trust Region Method not clip or pspo: {self.trust_region_method}')
 
-        per_token_loss1 = coef_1 * advantages.unsqueeze(1)
-        per_token_loss2 = coef_2 * advantages.unsqueeze(1)
-        per_token_loss = -torch.min(per_token_loss1, per_token_loss2)
         if entropy_mask is not None:
             per_token_loss = per_token_loss * entropy_mask
 
